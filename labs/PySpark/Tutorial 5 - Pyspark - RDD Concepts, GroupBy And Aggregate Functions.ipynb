{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efa0fce",
   "metadata": {},
   "source": [
    "### Pyspark - RDD Concepts, GroupBy And Aggregate Functions\n",
    "\n",
    "Jay Urbain, PhD   \n",
    "4/26/2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474b95e",
   "metadata": {},
   "source": [
    "![](spark-cluster-overview.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b8e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "JAVA_HOME = \"C:\\Program Files\\Java\\jdk-17.0.1\"\n",
    "os.environ[\"JAVA_HOME\"] = JAVA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f336300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3de8b",
   "metadata": {},
   "source": [
    "In order to create an RDD, first, you need to create a SparkSession which is an entry point to the PySpark application. SparkSession can be created using a builder() or newSession() methods of the SparkSession.\n",
    "\n",
    "Spark session internally creates a sparkContext variable of SparkContext. You can create multiple SparkSession objects but only one SparkContext per JVM. In case if you want to create another new SparkContext you should stop existing Sparkcontext (using stop()) before creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23513a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Agg').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "248b3a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MSOE-PF32JBSE.ad.msoe.edu:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Agg</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24602518760>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308375d",
   "metadata": {},
   "source": [
    "#### PySpark RDD – Resilient Distributed Dataset\n",
    "\n",
    "PySpark RDD (Resilient Distributed Dataset) is a fundamental data structure of PySpark that is fault-tolerant, immutable distributed collections of objects, which means once you create an RDD you cannot change it. Each dataset in RDD is divided into logical partitions, which can be computed on different nodes of the cluster.\n",
    "\n",
    "In order to create an RDD, we need to create a SparkSession which is an entry point to the PySpark application. SparkSession can be created using a builder() or newSession() methods of the SparkSession.\n",
    "\n",
    "Spark session internally creates a sparkContext variable of SparkContext. We can create multiple SparkSession objects but only one SparkContext per JVM. In case if you want to create another new SparkContext you should stop existing Sparkcontext (using stop()) before creating a new one.\n",
    "\n",
    "If you have tabular data, you should just use a DataFrame. Going down to the RDD layer is helpful for unstructured data, for example, text, images, and signal data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68bfc3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from list using parallelize\n",
    "\n",
    "dataList = [(\"Java\", 2000), (\"Python\", 10000), (\"Scala\", 3000)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab3db85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from external data source\n",
    "\n",
    "dataList = [(\"Java\", 2000), (\"Python\", 10000), (\"Scala\", 3000)]\n",
    "rdd2=spark.sparkContext.textFile('test3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed6295",
   "metadata": {},
   "source": [
    "#### RDD Operations\n",
    "\n",
    "On PySpark RDD, you can perform two kinds of operations.\n",
    "\n",
    "RDD transformations – Transformations are lazy operations. When you run a transformation(for example update), instead of updating a current RDD, these operations return another RDD.\n",
    "\n",
    "RDD actions – operations that trigger computation and return RDD values to the driver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf741fb",
   "metadata": {},
   "source": [
    "#### RDD Transformations\n",
    "\n",
    "Transformations on Spark RDD returns another RDD and transformations are lazy meaning they don’t execute until you call an action on RDD. Some transformations on RDD’s are flatMap(), map(), reduceByKey(), filter(), sortByKey() and return new RDD instead of updating the current."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aef9a9",
   "metadata": {},
   "source": [
    "#### PySpark DataFrame\n",
    "\n",
    "DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with  optimizations under the hood. DataFrames can be constructed from a wide array of sources such as structured data files, tables in Hive, external databases, or existing RDDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7b537",
   "metadata": {},
   "source": [
    "**TODO: Is PySpark faster than pandas? Explain your thinking.**\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11432d7",
   "metadata": {},
   "source": [
    "Create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3bd081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test3.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ed791ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d57d24ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405ebe6",
   "metadata": {},
   "source": [
    "PySpark SQL is one of the most used PySpark modules which is used for processing structured columnar data format. Once you have a DataFrame created, you can interact with the data by using SQL syntax.\n",
    "\n",
    "In other words, Spark SQL brings native SQL queries on Spark meaning you can run traditional ANSI SQL’s on Spark Dataframe, in the later section of this PySpark SQL tutorial, you will learn in detail using SQL select, where, group by, join, union e.t.c\n",
    "\n",
    "In order to use SQL, first, create a temporary table on DataFrame using createOrReplaceTempView() function. Once created, this table can be accessed throughout the SparkSession using sql() and it will be dropped along with your SparkContext termination.\n",
    "\n",
    "Use sql() method of the SparkSession object to run the query and this method returns a new DataFrame.\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed27207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.createOrReplaceTempView(\"PEOPLE\")\n",
    "df1=spark.sql(\"select Name, Departments, Salary as salary from PEOPLE\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f15f8197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Groupby\n",
    "\n",
    "# Grouped to find the maximum salary\n",
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b774a1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform the operation above using PySpark SQL\n",
    "df2 = spark.sql(\"SELECT Name, SUM(salary) FROM PEOPLE GROUP BY Name\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc122ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     Name|       avg(salary)|\n",
      "+---------+------------------+\n",
      "|Sudhanshu|11666.666666666666|\n",
      "|    Sunny|            6000.0|\n",
      "|    Krish| 6333.333333333333|\n",
      "|   Mahesh|            3500.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "151d2264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use PySpark dataframe to group by departmebnnts and compute sum for each department\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "df_pyspark.withColumn(\"salary\", col(\"salary\").cast(\"integer\")).groupBy('Departments').agg(sum('salary')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc856adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use PySpark SQL to group by departmebnnts and compute sum for each department\n",
    "df3=spark.sql(\"SELECT Departments, SUM(salary) FROM PEOPLE GROUP BY Departments\")\n",
    "df3.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "826d5c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use PySpark dataframe to group and compute count for each department\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "df_pyspark.groupBy('Departments').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb50fa0",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37b26cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21f03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
